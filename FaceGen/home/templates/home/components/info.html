{% load static%}

<div id="modal" class="fixed z-50 left-0 top-0 w-full h-full overflow-auto bg-gray-600 bg-opacity-90">
    
    <div class="bg-[--primary-color] h-[80dvh] overflow-y-auto p-5 m-20 self-center justify-center border border-gray-300 shadow-lg rounded-lg">
        <span class="close cursor-pointer float-right text-l" @click="$($el).parent().parent().remove()">
            <svg width="32px" height="32px" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path d="M17.2929 5.29289C17.6834 4.90237 18.3166 4.90237 18.7071 5.29289C19.0976 5.68342 19.0976 6.31658 18.7071 6.70711L13.4142 12L18.7071 17.2929C19.0976 17.6834 19.0976 18.3166 18.7071 18.7071C18.3166 19.0976 17.6834 19.0976 17.2929 18.7071L12 13.4142L6.70711 18.7071C6.31658 19.0976 5.68342 19.0976 5.29289 18.7071C4.90237 18.3166 4.90237 17.6834 5.29289 17.2929L10.5858 12L5.29289 6.70711C4.90237 6.31658 4.90237 5.68342 5.29289 5.29289C5.68342 4.90237 6.31658 4.90237 6.70711 5.29289L12 10.5858L17.2929 5.29289Z" fill="#000000"/>
            </svg>
        </span>
        {% if method == "dreambooth" %}
            <div class="flex items-center w-full mb-6">
                <h1 class="text-5xl font-light">Dreambooth</h1>
            </div>

            <section class="mb-8 w-full">
                <h2 class="text-2xl font-semibold mb-4">Technologies Used</h2>
                <p class="text-lg mb-4">Dreambooth is a technique used to fine-tune generative models on a specific set of images, enabling personalized image generation based on new textual prompts.</p>
                
                <h3 class="text-xl font-medium mb-4">What is Dreambooth?</h3>
                <p class="text-lg mb-4">Dreambooth allows for customization of generative models by learning specific visual concepts from a small dataset of images. This technique fine-tunes a pre-trained model to generate images that closely match the personalized concepts present in the provided dataset.</p>
                
                <h3 class="text-xl font-medium mb-2">How It Works:</h3>
                <ul class="list-disc list-inside mb-4 text-lg">
                    <li>Data Collection: A small dataset of images related to a specific theme or subject is collected. These images should represent the visual concepts you want the model to learn.</li>
                    <li>Model Fine-Tuning: The pre-trained generative model, such as stable-diffusion-v1-5, is fine-tuned using the collected dataset. This involves retraining the model on the new images to adapt its weights to the new visual concepts.</li>
                    <li>Concept Learning: During fine-tuning, the model learns to associate specific visual characteristics and styles from the dataset with the corresponding textual descriptions.</li>
                    <li>Personalized Generation: After fine-tuning, the model can generate new images that are personalized and aligned with the concepts learned during the training process.</li>
                </ul>

                <h3 class="text-xl font-medium mb-2">Detailed Technical Explanation:</h3>
                <ul class="list-disc list-inside mb-4 text-lg">
                    <li>Architecture: Dreambooth typically fine-tunes transformer-based models, such as UNet architectures, that are capable of handling complex patterns in both image and text data. The architecture includes multiple layers of self-attention mechanisms to capture detailed relationships between different parts of the input data.</li>
                    <li>Training Data: The fine-tuning process uses a small but carefully selected dataset. This dataset should cover a range of variations in the subject matter to ensure the model can generalize well. Data augmentation techniques may be used to artificially increase the diversity of the dataset.</li>
                    <li>Training Process: The fine-tuning process involves adjusting the model's weights using a loss function that measures the difference between the generated images and the target images. This typically requires fewer training iterations compared to training from scratch, making it computationally efficient.</li>
                    <li>Capabilities: The fine-tuned model can generate highly personalized images that reflect the visual style and characteristics of the training dataset. This is particularly useful for applications requiring custom content generation, such as personalized avatars, art, or product designs.</li>
                </ul>
            </section>

        {% elif method == "stable_diffusion" %}
            <div class="flex items-center w-full mb-6">
                <h1 class="text-5xl font-light">Stable Diffusion</h1>
            </div>

            <section class="mb-8 w-full">
                <h2 class="text-2xl font-semibold mb-4">Technologies Used</h2>
                <p class="text-lg mb-4">Stable Diffusion Pipelines are used to generate images from textual descriptions. These pipelines utilize stable diffusion models, a type of generative model that iteratively refines images to match given textual prompts.</p>
                
                <h3 class="text-xl font-semibold mb-4">What is a Pipeline?</h3>
                <p class="text-lg mb-4">A pipeline is a series of data processing steps that are executed sequentially. Each step in the pipeline takes an input, processes it, and provides an output that becomes the input for the next step. Pipelines are commonly used in machine learning and data processing to structure and automate workflows.</p>
                <h3 class="text-xl font-medium mb-2">How It Works:</h3>
                <ul class="list-disc list-inside mb-4 text-lg">
                    <li>Text Encoding: The textual description is encoded into a latent space.</li>
                    <li>Image Generation: The encoded text guides the generation process, starting from random noise and iteratively refining the image.</li>
                    <li>Diffusion Process: Each step in the diffusion process reduces noise and enhances the image quality to better match the text description.</li>
                </ul>

                <h3 class="text-xl font-medium mb-2">Model</h3>
                <p class="text-lg mb-4">The runwayml/stable-diffusion-v1-5 model is a pre-trained image generation model available on Hugging Face. It is specifically designed for generating high-quality images based on textual inputs.</p>

                <h3 class="text-xl font-medium mb-2">In-Depth Technical Explanation:</h3>
                <ul class="list-disc list-inside mb-4 text-lg">
                    <li>Architecture: The model utilizes a transformer-based architecture, specifically a variant called the UNet (Universal Network). This architecture is adept at capturing complex relationships and patterns in both text and image data. It includes multiple layers of self-attention mechanisms that allow the model to focus on different parts of the input text while generating the corresponding parts of the image.</li>
                    <li>Training Data: The model is trained on a large and diverse dataset comprising millions of image-text pairs. This dataset includes a wide variety of scenes, objects, and styles, which allows the model to learn to generalize well to new prompts. The training process involves optimizing the model parameters using a loss function that measures the difference between the generated images and the target images.</li>
                    <li>Capabilities: The model is capable of generating highly detailed and high-resolution images. It can capture fine-grained details and textures, making it ideal for applications like FaceGen where image quality is paramount. The model's ability to understand and generate complex visual concepts from textual descriptions enables it to produce realistic and aesthetically pleasing images.</li>
                </ul>
            </section>

            <section class="mb-8 w-full">
                <h2 class="text-2xl font-semibold mb-4">User Manual</h3>
                <h3 class="text-xl font-medium mb-2">Home Page</h3>
                <p class="text-lg mb-4">The home page is the main page of FaceGen where users can interact with the platform.</p>

                <h3 class="text-xl font-medium mb-2">Features:</h3>
                <ul class="list-disc list-inside mb-4 text-lg">
                    <li>Prompt Input Field: Allows the user to write a textual description of the image they want to generate (e.g., "A dog playing in a park").</li>
                    <li>Send Button: Sends the prompt to generate an image.</li>
                    <li>Settings Button: Allows users to adjust image parameters before sending the prompt.</li>
                </ul>

                <h3 class="text-xl font-medium mb-2">Image Parameters:</h3>
                <ul class="list-disc list-inside mb-4 text-lg">
                    <li>Negative Prompt: Allows excluding certain elements from the generated image. Example: If you do not want trees in the image, you can add "trees" in the negative prompt.</li>
                    <li>Guidance Scale: Controls the influence of the text on the generated image. Values: A higher value makes the image more faithful to the textual description, while a lower value allows for more creativity.</li>
                    <li>Number of Inference Steps: Defines the number of steps used to generate the image. Values: More steps can improve the image quality but also increase the generation time.</li>
                    <li>Seed: Allows reproducing a specific image by using the same seed. Values: Using the same seed value with the same parameters will always generate the same image.</li>
                </ul>

                <h3 class="text-xl font-medium mb-2">History:</h3>
                <ul class="list-disc list-inside mb-4 text-lg">
                    <li>Image Display: Generated images are automatically saved in the history section at the bottom of the page.</li>
                    <li>Trash Button: Allows deleting an image from the history.</li>
                </ul>
            </section>

            <section class="mb-8 w-full">
                <h2 class="text-2xl font-semibold mb-4">Frequently Asked Questions</h3>
                <ul class="list-disc list-inside mb-4 text-lg">
                    <li class="underline">Q1: What types of descriptions work best?</li>
                    <p class="pl-4 font-semibold">A1: Detailed and specific descriptions yield the best results.</p>
                    <li class="underline">Q2: How can I improve the generated images?</li>
                    <p class="pl-4 font-semibold">A2: Use clear and descriptive language, and consider providing additional context or constraints.</p>
                    <li class="underline">Q3: Can I use my own images for fine-tuning?</li>
                    <p class="pl-4 font-semibold">A3: Yes, you can use the Dreambooth technique to fine-tune the model on your own images.</p>
                </ul>
            </section>
        {% endif %}
    </div>
</div>
